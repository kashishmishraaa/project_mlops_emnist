{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a707672d",
   "metadata": {},
   "source": [
    "# Loading the data using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97404fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562M/562M [05:56<00:00, 1.57MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 697932\n",
      "Testing Samples: 116323\n",
      "Number of Classes: 62\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ✅ Load EMNIST Dataset (ByClass) using PyTorch\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "#Updated split='byclass' to load all 62 character classes (digits + uppercase/lowercase letters).\n",
    "train_dataset = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Testing Samples: {len(test_dataset)}\")\n",
    "print(f\"Number of Classes: {len(train_dataset.classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3e0be",
   "metadata": {},
   "source": [
    "#save training and testing data in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75004243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST dataset saved as CSV files: emnist_train.csv & emnist_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ✅ Convert PyTorch Tensors to NumPy arrays\n",
    "x_train = train_dataset.data.numpy().reshape(-1, 28*28)  # Flatten images (28x28 → 784 pixels)\n",
    "y_train = train_dataset.targets.numpy()\n",
    "\n",
    "x_test = test_dataset.data.numpy().reshape(-1, 28*28)\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# ✅ Create Pandas DataFrames\n",
    "df_train = pd.DataFrame(x_train)\n",
    "df_train.insert(0, \"label\", y_train)  # Add label column\n",
    "\n",
    "df_test = pd.DataFrame(x_test)\n",
    "df_test.insert(0, \"label\", y_test)\n",
    "\n",
    "# ✅ Save as CSV files\n",
    "df_train.to_csv(\"emnist_train.csv\", index=False)\n",
    "df_test.to_csv(\"emnist_test.csv\", index=False)\n",
    "\n",
    "print(\"EMNIST dataset saved as CSV files: emnist_train.csv & emnist_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b8620",
   "metadata": {},
   "source": [
    "#checking for missing or null values in full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea80c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows (samples): 814255\n",
      "Number of columns (features per sample): 784\n",
      "No missing or invalid values found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the train and test datasets\n",
    "train_dataset = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine datasets\n",
    "full_dataset = train_dataset + test_dataset\n",
    "\n",
    "# ✅ Find number of rows and columns\n",
    "num_rows = len(full_dataset)  # Total number of samples\n",
    "sample_data, _ = full_dataset[0]  # Take one sample to find shape\n",
    "num_columns = sample_data.numel()  # Flattened number of pixels (channels × height × width)\n",
    "\n",
    "print(f\"Number of rows (samples): {num_rows}\")\n",
    "print(f\"Number of columns (features per sample): {num_columns}\")\n",
    "\n",
    "# Check for missing or invalid values\n",
    "for i, (data, label) in enumerate(full_dataset):\n",
    "    if torch.isnan(data).any() or torch.isinf(data).any() or label is None:\n",
    "        print(f\"Missing or invalid value found at index {i}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No missing or invalid values found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96e89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training dataset: 697932\n",
      "Number of columns in training dataset: 784\n",
      "Number of rows in testing dataset: 116323\n",
      "Number of columns in testing dataset: 784\n"
     ]
    }
   ],
   "source": [
    "# ✅ Find number of rows and columns in training dataset\n",
    "num_rows = len(train_dataset)  # Total number of samples\n",
    "sample_data, _ = train_dataset[0]  # Take one sample to find shape\n",
    "num_columns = sample_data.numel()  # Flattened number of pixels (channels × height × width)\n",
    "\n",
    "print(f\"Number of rows in training dataset: {num_rows}\")\n",
    "print(f\"Number of columns in training dataset: {num_columns}\")\n",
    "\n",
    "# ✅ Find number of rows and columns in testing dataset\n",
    "num_rows = len(test_dataset)  # Total number of samples\n",
    "sample_data, _ = test_dataset[0]  # Take one sample to find shape\n",
    "num_columns = sample_data.numel()  # Flattened number of pixels (channels × height × width)\n",
    "\n",
    "print(f\"Number of rows in testing dataset: {num_rows}\")\n",
    "print(f\"Number of columns in testing dataset: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3188b",
   "metadata": {},
   "source": [
    "## Reduce size: Select only first 90,000 samples for training, 10,000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f3f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced training dataset size: 90000 samples\n",
      "Reduced testing dataset size: 10000 samples\n",
      "Each sample shape: torch.Size([1, 28, 28]) => Number of columns (flattened): 784\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "small_train_dataset = Subset(train_dataset, range(90000))\n",
    "small_test_dataset = Subset(test_dataset, range(10000))\n",
    "\n",
    "# Print reduced dataset sizes\n",
    "print(f\"Reduced training dataset size: {len(small_train_dataset)} samples\")\n",
    "print(f\"Reduced testing dataset size: {len(small_test_dataset)} samples\")\n",
    "\n",
    "# Check number of features (columns)\n",
    "sample_data, _ = small_train_dataset[0]\n",
    "print(f\"Each sample shape: {sample_data.shape} => Number of columns (flattened): {sample_data.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd324ce6",
   "metadata": {},
   "source": [
    "#remove duplicates values , if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70af049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Removed duplicate rows (if any).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract data and labels from the original EMNIST dataset using the indices from small_train_dataset and small_test_dataset\n",
    "train_data = train_dataset.data[small_train_dataset.indices]\n",
    "train_labels = train_dataset.targets[small_train_dataset.indices]\n",
    "\n",
    "test_data = test_dataset.data[small_test_dataset.indices]\n",
    "test_labels = test_dataset.targets[small_test_dataset.indices]\n",
    "\n",
    "# Convert the small datasets into DataFrames for easy manipulation\n",
    "df_train = pd.DataFrame(train_data.numpy().reshape(-1, 784))  # Reshape the data into 784 columns (flattened images)\n",
    "df_train['label'] = train_labels.numpy()  # Add the labels\n",
    "\n",
    "df_test = pd.DataFrame(test_data.numpy().reshape(-1, 784))  # Reshape the data into 784 columns (flattened images)\n",
    "df_test['label'] = test_labels.numpy()  # Add the labels\n",
    "\n",
    "# Remove duplicates\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"✅ Removed duplicate rows (if any).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6ae0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor()  # Convert images to tensors\n",
    "])\n",
    "\n",
    "train_dataset = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06578c6",
   "metadata": {},
   "source": [
    "## normaliza data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad4226e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Training Data (First 5 Rows):\n",
      "    0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
      "0  0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   779  780  781  782  783     label  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.000538  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.000554  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.000092  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.000046  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.000338  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "✅ Data Normalized: Pixel values are now between 0 and 1.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Normalize pixel values (0 to 1 range)\n",
    "df_train.iloc[:, 1:] = df_train.iloc[:, 1:] / 255.0  # Normalize all pixel columns\n",
    "df_test.iloc[:, 1:] = df_test.iloc[:, 1:] / 255.0\n",
    "# ✅ Print only the first 5 rows to verify\n",
    "print(\"Sample Training Data (First 5 Rows):\\n\", df_train.head())\n",
    "print(\"✅ Data Normalized: Pixel values are now between 0 and 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20a7ff",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Data for CNN Model\n",
    "CNNs need image data in shape (28, 28, 1) (not flattened). So, let’s reshape it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8572ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (90000, 28, 28, 1)\n",
      "y_train shape: (90000, 62)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000, 62)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Access the original EMNIST dataset inside the Subset object\n",
    "x_train = small_train_dataset.dataset.data[small_train_dataset.indices].numpy().reshape(-1, 28, 28, 1)  # Reshape the data into 28x28 images with 1 channel\n",
    "y_train = small_train_dataset.dataset.targets[small_train_dataset.indices].numpy()  # Get the labels\n",
    "\n",
    "x_test = small_test_dataset.dataset.data[small_test_dataset.indices].numpy().reshape(-1, 28, 28, 1)  # Reshape the data into 28x28 images with 1 channel\n",
    "y_test = small_test_dataset.dataset.targets[small_test_dataset.indices].numpy()  # Get the labels\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes=62)\n",
    "y_test = to_categorical(y_test, num_classes=62)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b4b5c",
   "metadata": {},
   "source": [
    "# define a simple cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c097f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,998</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)             │         \u001b[38;5;34m7,998\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,742</span> (905.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m231,742\u001b[0m (905.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,742</span> (905.24 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m231,742\u001b[0m (905.24 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the input layer explicitly\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "\n",
    "# Define the rest of the model\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output_layer = Dense(62, activation='softmax')(x)  # 62 classes for EMNIST ByClass\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e22076",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cd6885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.7195 - loss: 0.9080 - val_accuracy: 0.8047 - val_loss: 0.5844\n",
      "Epoch 2/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.7741 - loss: 0.6875 - val_accuracy: 0.8180 - val_loss: 0.5293\n",
      "Epoch 3/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.8013 - loss: 0.5945 - val_accuracy: 0.8243 - val_loss: 0.5132\n",
      "Epoch 4/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.8128 - loss: 0.5531 - val_accuracy: 0.8287 - val_loss: 0.4998\n",
      "Epoch 5/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.8189 - loss: 0.5253 - val_accuracy: 0.8301 - val_loss: 0.5027\n",
      "Epoch 6/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 20ms/step - accuracy: 0.8268 - loss: 0.4958 - val_accuracy: 0.8357 - val_loss: 0.4863\n",
      "Epoch 7/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.8326 - loss: 0.4704 - val_accuracy: 0.8332 - val_loss: 0.4779\n",
      "Epoch 8/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - accuracy: 0.8365 - loss: 0.4529 - val_accuracy: 0.8361 - val_loss: 0.4933\n",
      "Epoch 9/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - accuracy: 0.8427 - loss: 0.4370 - val_accuracy: 0.8406 - val_loss: 0.4843\n",
      "Epoch 10/10\n",
      "\u001b[1m1266/1266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 62ms/step - accuracy: 0.8429 - loss: 0.4289 - val_accuracy: 0.8352 - val_loss: 0.4874\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
